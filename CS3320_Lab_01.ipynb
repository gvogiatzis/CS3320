{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS3320_Lab_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgndP5Ygkah6L/bZiPHZ84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS3320/blob/main/CS3320_Lab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS3320 Lab 1. Boolean Retrieval"
      ],
      "metadata": {
        "id": "DESpApL3wrI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this lab we will explore some of the fundamental information retrieval concepts we saw in the lectures including indexing, ranked and boolean retrieval. You are given a small dataset of 737 news stories scraped from the **BBC Sports website** between 2004 and 2005 (full dataset [here](http://mlg.ucd.ie/datasets/bbc.html)). Your task is to write a basic search engine in Python, using the Boolean Retrieval model we talked about in lectures. \n",
        "Your engine should consist of an indexer which processes all documents and creates an index as well a query engine that takes a query string and returns a list of documents that contains all the terms."
      ],
      "metadata": {
        "id": "K7Mx8ns3xMFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/gvogiatzis/CS3320/raw/main/data/bbc_sport_docs.zip"
      ],
      "metadata": {
        "id": "ChgJzK-b6NJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip bbc_sport_docs.zip -d docs"
      ],
      "metadata": {
        "id": "UXXQjfFe8P2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The dataset\n",
        "Before we do anything else, let's have a look at the dataset I will assume you will have downloaded from blackboard the dataset *bbcsport.tar.gz* and unzipped it (`gunzip bbcsport.tar.gz` followed by `tar -xf bbcsport.tar`) in a convenient location. Go into that directory and have a look at some of the files using the `cat` command, i.e."
      ],
      "metadata": {
        "id": "SNU8cnntxTie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat docs/000.txt"
      ],
      "metadata": {
        "id": "lzD4D5Cg9Adk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "should produce something like\n",
        "\n",
        "    McCall earns Tannadice reprieve\n",
        "\n",
        "    Dundee United manager Ian McCall has won a reprieve from the sack, with chairman Eddie Thompson calling for an end to speculation over his future...etc\n",
        "\n",
        "Now let's try to open and print the file in python. First, you need to import these modules that will be used throughout"
      ],
      "metadata": {
        "id": "VGI6nHcM_pHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "mMBr7ouixYbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can open a file using the built-in `open` command as follows:"
      ],
      "metadata": {
        "id": "oEbfVZE-AHro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('docs/000.txt','r', encoding='latin-1')"
      ],
      "metadata": {
        "id": "NFXUK7OnAGPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method f.read() then reads the file into a string as follows"
      ],
      "metadata": {
        "id": "xc1vEkXWCWsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = f.read()"
      ],
      "metadata": {
        "id": "ECF-O2O1CUjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try printing the first 200 characters to see how the output looks. "
      ],
      "metadata": {
        "id": "EoptQ57ICabX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(s[0:199])"
      ],
      "metadata": {
        "id": "oPoqTFmrCYdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the `os.listdir()` command to obtain a list of all the filenames under a directory and then open a specific one using an index into that list. This index will become our document ID that our search engine will use. All that can be neatly placed inside a function for printing files as follows:"
      ],
      "metadata": {
        "id": "Ulj4_d2_CeiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readfile(path, docid):\n",
        "    files = sorted(glob(path))\n",
        "    f = open(files[docid], 'r', encoding='latin-1')\n",
        "    s = f.read()\n",
        "    f.close()\n",
        "    return s"
      ],
      "metadata": {
        "id": "yH8DLPvtCcJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try using this function on the `docs` path for various docid's. So we now know how to open a file and turn its contents into a string. The next step is chopping that string up in tokens.\n"
      ],
      "metadata": {
        "id": "NEgkQ8FcClLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "Fortunately tokenization is a very simple task in Python because we can use the built-in `split` method for strings. Let's try reading a file into a string, split it and then print the first 20 tokens."
      ],
      "metadata": {
        "id": "NGdkmcP-CouU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = readfile('docs/*.txt', 0)\n",
        "tokens = s.split()\n",
        "print(tokens[0:19])"
      ],
      "metadata": {
        "id": "3TEniYEoCqbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the simple `split` method in strings only splits on whitespace characters leaving punctuation marks (as well as numbers, hyphens etc) which is why it returned \"`sack,`\" as a token. A little digging points us to the direction of the Regular Expression module (`re`) and the split method contained therein, which can accept a whole list of delimiter characters in the form of a delimiter string. Let's try this"
      ],
      "metadata": {
        "id": "qdkKL8e2C1iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DELIM = '[ \\n\\t0123456789;:.,/\\(\\)\\\"\\'-]+'\n",
        "tokens = re.split(DELIM, s)\n",
        "print(tokens[0:19])"
      ],
      "metadata": {
        "id": "iUrJCsevCsL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks better. Now all we need to do is turn these into lowercase. It's better to do this on the entire string before splitting it:"
      ],
      "metadata": {
        "id": "saxYVqZwC_Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DELIM = '[ \\r\\n\\t0123456789;:.,/\\(\\)\\\"\\'-]+'\n",
        "s_lower = s.lower()\n",
        "tokens = re.split(DELIM, s_lower)\n",
        "print(tokens[0:19])"
      ],
      "metadata": {
        "id": "nKuGitzcC4Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All this can be compacted in a single line of Python code and wrapped inside a function:"
      ],
      "metadata": {
        "id": "AAVIKeHnDGdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return re.split(DELIM, text.lower())"
      ],
      "metadata": {
        "id": "JxhT70AjDDGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boolean retrieval - Indexing\n",
        "In this section we will look at Boolean retrieval, as a warming up excercise before you tackle ranked retrieval on your own! The first step in a boolean search engine is to build the indexer. This piece of code is responsible for reading all the documents and producing the postings lists which, as we saw in the lectures, is an efficient way of storing the term-document incidence matrix.\n",
        "\n",
        "A nice data-structure for storing postings lists is a Python dictionary which as we have seen can be indexed by strings. So we want to produce a dictionary whose keys are words found in the documents and whose values are *sets* of docid's of documents that contain those words. So a postings list such as\n",
        "    \n",
        "    {'cricket': {2, 3, 5, 7}, 'football': {0, 2, 4}, 'rugby': {1, 2, 6}}\n",
        "    \n",
        "would denote that the word *cricket* can be found in documents with id's 2, 3, 5 and 7 etc. So how to construct this postings dictionary? Well let us read and tokenize the file with id 0 once again."
      ],
      "metadata": {
        "id": "8qaKJkXdDKxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = readfile('docs/*.txt', 0)\n",
        "words = tokenize(s)"
      ],
      "metadata": {
        "id": "1WIRsCXjDIPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sideffect of the `re.split` function is that it occasionally returns an empty string among the rest of the tokens, if the string ends with one of the delimiter characters. To avoid that we can just remove the empty string if it is in the returned tokens"
      ],
      "metadata": {
        "id": "Fx4rXvtmDSpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if '' in words:\n",
        "    words.remove('')"
      ],
      "metadata": {
        "id": "cWnfRwpLDQpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now starting with an empty dictionary we will add all the words in `words`. If the word is not contained in the dictionary we create a singleton set with the doc-id 0. If the word is already contained in the dictionary we only add 0 to the corresponding set of docid's. This looks as follows: "
      ],
      "metadata": {
        "id": "L19rgx8zDU1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "postings = {}\n",
        "for w in words:\n",
        "    postings.setdefault(w, set()).add(0)"
      ],
      "metadata": {
        "id": "VQHJuPuADUYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print out the `postings` dictionary to see what it contains. You should get a dictionary with keys equal to all the words contained in our doc and values {0}. \n",
        "\n",
        "We can now do this for the whole collection of documents, and encapsulate the whole indexing engine in a function as follows:"
      ],
      "metadata": {
        "id": "_lvUd3LaDcD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indextextfiles_BR(path):\n",
        "    N = len(sorted(glob(path)))\n",
        "    postings={}         \n",
        "    for docID in range(N):\n",
        "        s = readfile(path, docID)\n",
        "        words = tokenize(s)\n",
        "        for w in words:\n",
        "            if w!='':\n",
        "                postings.setdefault(w, set()).add(docID)\n",
        "    return postings"
      ],
      "metadata": {
        "id": "2gplVnF5DaPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So to process the entire directory and generate the complete postings dictionary we can execute:"
      ],
      "metadata": {
        "id": "gc3DjMf5DhNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "postings = indextextfiles_BR('docs/*.txt')"
      ],
      "metadata": {
        "id": "5qk0schcDf79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use this datastructure to find out which documents contain the word '`devastating`'. We just need to execute:"
      ],
      "metadata": {
        "id": "7_1sTaeVEgrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(postings['devastating'])"
      ],
      "metadata": {
        "id": "FM-Z4mabEW6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "which tells us that the word 'devastating' is contained in docs with id's 195, 310 and 55. Neat!"
      ],
      "metadata": {
        "id": "Zdd7chMqEl3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing boolean queries\n",
        "We are now ready to process a boolean query. We will be assuming that the user gives a string containing all the query terms separated by spaces and they will expect all documents that contain all those query terms. Let's begin by assuming the query text '`england football defeat`'"
      ],
      "metadata": {
        "id": "8gvTG1y9Epjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qtext = 'england football defeat'"
      ],
      "metadata": {
        "id": "pwRX78B1EiWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by tokenizing it"
      ],
      "metadata": {
        "id": "8j-8OuA9E9GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = tokenize(qtext)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "qid-ABRbExBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform boolean retrieval we need to get the postings of each of the three terms and then take their intersection (`&` operator in Python) as follows:"
      ],
      "metadata": {
        "id": "y_8nKVxdE_eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = postings['england']\n",
        "p2 = postings['football']\n",
        "p3 = postings['defeat']\n",
        "print(p1&p2&p3)"
      ],
      "metadata": {
        "id": "2QATSQo5E_sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More generally, we can iterate through the list of terms in the query, grab the postings sets for each of them and take their intersection. This can be done inside a function as follows. "
      ],
      "metadata": {
        "id": "9dg6SL1JFDWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_BR(postings, qtext):\n",
        "    words = tokenize(qtext)\n",
        "    res = None\n",
        "    for w in words:\n",
        "        res = postings[w] if res==None else res & postings[w]\n",
        "    return res"
      ],
      "metadata": {
        "id": "XfuxgooGFBGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the check inside the for-loop for the very first time through the loop where `res` has not been set yet. If you want, you can try a more elegant (or Pythonic as people call it) way of doing the same thing is using `set.intersection` and the `*` operator that turns a list into arguments for a function. "
      ],
      "metadata": {
        "id": "qfzyIH-OFIzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_BR(postings, qtext):\n",
        "    words = tokenize(qtext)\n",
        "    allpostings = [postings[w] for w in words]\n",
        "    res = set.intersection(*allpostings)\n",
        "    return res"
      ],
      "metadata": {
        "id": "xuBdRk5zFFYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we get the same results"
      ],
      "metadata": {
        "id": "evhjGjf9FMOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_BR(postings,'england football defeat')"
      ],
      "metadata": {
        "id": "7oL7R7QxFKeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice!"
      ],
      "metadata": {
        "id": "8pxWLdB-MbyI"
      }
    }
  ]
}