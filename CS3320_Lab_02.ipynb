{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS3320_Lab_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6XLFmqahdnP8TAvnGx+oj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS3320/blob/main/CS3320_Lab_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS3320 Lab 2. Ranked Retrieval"
      ],
      "metadata": {
        "id": "0_BiVXu8KlvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this lab we will further explore our **BBC Sports** dataset by applying some ranked retrieval algorithms. In the process we will look at TF-IDF scoring and perform some queries with free text. Our search engine should use the following formula for the tf-idf weightings\n",
        "\n",
        "$$tfidf_{t,d}=tf_{t,d}\\times\\log_{10}\\left(\\frac{N}{df_{t}}\\right)$$ \n",
        "\n",
        "where $tf_{t,d}$ is the number of times term $t$ appears in document $d$ and $df_{t}$ is the number of documents that contain term $t$ and $N$ is the total number of documents in the collection. We should also normalize your documents but not the query. \n",
        "\n",
        "Let's download the files again and unzip them in the google colab virtual computer drive."
      ],
      "metadata": {
        "id": "1kxs7o9LKs7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/gvogiatzis/CS3320/raw/main/data/bbc_sport_docs.zip\n",
        "!unzip bbc_sport_docs.zip -d docs"
      ],
      "metadata": {
        "id": "ChgJzK-b6NJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import some toolboxes again."
      ],
      "metadata": {
        "id": "FCRndErIP90N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from glob import glob\n",
        "from math import log10"
      ],
      "metadata": {
        "id": "kanUh82QPazR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We re-run the code from the previous lab that reads the files and does the tokenization. None of that needs to change."
      ],
      "metadata": {
        "id": "uAa9plZYQBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readfile(fname):\n",
        "    f = open(fname, 'r', encoding='latin-1')\n",
        "    s = f.read()\n",
        "    f.close()\n",
        "    return s\n",
        "\n",
        "def tokenize(text):\n",
        "    DELIM = '[ \\r\\n\\t0123456789;:.,/\\(\\)\\\"\\'-]+'\n",
        "    return re.split(DELIM, text.lower())"
      ],
      "metadata": {
        "id": "yH8DLPvtCcJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Term frequencies\n",
        "\n",
        "In the Boolean Retrieval model (Lab 01) we saw how the postings datastructure was essentially a mapping from terms in the dictionary, to the document id's that contain that term. A mapping of this type can elegantly be represented in Python using a *dictionary*. So a postings dictionary such as\n",
        "    \n",
        "    {'cricket': {2, 3, 5, 7}, 'football': {0, 2, 4}, 'rugby': {1, 2, 6}}\n",
        "    \n",
        "would denote that the word *cricket* can be found in documents with id's 2, 3, 5 and 7 etc. \n",
        "\n",
        "Now the only trouble is that for ranked retrieval we need the *number of times* a particular term appears in a particular document and the postings structure described above does not provide that information. What can we do instead? \n",
        "\n",
        "For ranked retrieval we would need a datastructure which maps a term and a document to a number, i.e. how many times that term is contained in that document. This could be achieved by something like:\n",
        "\n",
        "    {('cricket',2) : 5, ('cricket',3) : 10, ('football',4):20, ('football',5):30, ('football',6):40}\n",
        "\n",
        "However we can achieve the same thing much more efficiently by a double-mapping from term to document to number. Something like the following:\n",
        "\n",
        "    {'cricket':{2:5, 3:10}, 'football':{4:20,5:30,6:40}}\n",
        "\n",
        "This is essentialy a dictionary with the terms being the keys, and the items being dictionaries themselves. These second-level dictionaries map document id's to the number of times the term appears in them. \n",
        "\n",
        "Let's say we gather a list of all the docIDs where that term appears. Note this is a list that may have duplicates arising from the same term appearing multiple times in the same document. E.g. [2,2,2,3,3,3,3,4,4]. \n",
        "\n",
        "To summarise this list using document counts, we create an empty dictionary. Thenwe go through that list of docIDs and everytime we come across a docID we havent seen before, we set the entry to one. If a docID entry exists we increment it by one. \n",
        "The result should be  {2:3, 3:4, 4:2} for the list above. This dictionary says that docid 2 has been found 3 times, docid 3 has been found 4 times and docid 4 has been found 2 times. This is what that looks like in Python:\n"
      ],
      "metadata": {
        "id": "GtEJAr2-QlO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [2,2,2,3,3,3,3,4,4]\n",
        "\n",
        "c = dict()\n",
        "\n",
        "for x in a:\n",
        "    if x in c:\n",
        "        c[x] += 1\n",
        "    else:\n",
        "        c[x] = 1\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "id": "JxhT70AjDDGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code can be simplified somewhat by using a special datastructure known as a *defaultdict*. The difference to a normal dict is that any key that hasn't been given an entry yet, is assumed to have a default value (in our case 0). Using a defaultdict the code above would look like"
      ],
      "metadata": {
        "id": "j0NWmt9QORDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "a = [2,2,2,3,3,3,3,4,4]\n",
        "\n",
        "c = defaultdict(int)\n",
        "\n",
        "for x in a:\n",
        "    c[x] += 1\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "id": "HsIsGaUqOjvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "which is a bit more *pythonic*. Challenge: Python `collections` has a type of data structure object called a `Counter`. Can you use a `Counter` to create the structure above given the array `a`, in a **single line of code**? Just search for `collections.Counter` to figure out the correct usage."
      ],
      "metadata": {
        "id": "_v1tKx-gOrNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hPRynAoWEj3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to create a function that will generate term frequencies for us. The structure of the code will follow that of the boolean retrieval model."
      ],
      "metadata": {
        "id": "dWbfO8mrEkrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_structure(path):\n",
        "    fnames = sorted(glob(path)) # load all filenames\n",
        "    tf=defaultdict(lambda : defaultdict(int)) \n",
        "    for docID,fname in enumerate(fnames):\n",
        "        s = readfile(fname)\n",
        "        words = tokenize(s)\n",
        "        for w in words:\n",
        "            tf[w][docID] += 1    \n",
        "    return tf"
      ],
      "metadata": {
        "id": "Jv5t34x-XN1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see this in action:"
      ],
      "metadata": {
        "id": "00Cldsw2G9SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = create_tf_structure(\"docs/*.txt\")"
      ],
      "metadata": {
        "id": "THgQzfkMX173"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the box below, to print out the postings for the word \"incredible\"."
      ],
      "metadata": {
        "id": "LPl1qgh8F22q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-Ngi4fUyGEB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverse document frequencies"
      ],
      "metadata": {
        "id": "qJrG1UTzIem-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next task is to compute the second component of TF-IDF which is the Inverse Document Frequency. And to do that, a good idea would be to start with Document Frequency. I.e. for each word, the number of documents that contain it. Luckily this can be computed from the `tf` structure computed above. You see for a term `w`, the dictionary `tf` has exactly as many elements as the number of documents that contain `w`. So `len[tf[w]]` would give us exactly what we need:"
      ],
      "metadata": {
        "id": "P86azjf_GVmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = dict()\n",
        "\n",
        "for w in tf.keys():\n",
        "    df[w] = len(tf[w])"
      ],
      "metadata": {
        "id": "plhcWldSN6yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want, you can write this in a single line using Python comprehensions as follows:"
      ],
      "metadata": {
        "id": "XQGDfRX7HBZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = {w:len(tf[w]) for w in tf.keys()}"
      ],
      "metadata": {
        "id": "ND5PRnrNHJrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is briefer and arguably easier to interpret. Now for the IDF formula we require *N* the numbe of all documents. This can be obtained from "
      ],
      "metadata": {
        "id": "09-sqS-yHOwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(glob('docs/*.txt'))"
      ],
      "metadata": {
        "id": "P_tdSNtPMo1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the box below, please write the one-line python code for computing `idf`. [Hint: modify the `df` code appropriately."
      ],
      "metadata": {
        "id": "5n63a05jMrko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m4naZculNFAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the `idf` of the word \"unique\". Use the box below."
      ],
      "metadata": {
        "id": "Q1IjuDOpM8LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2P5ariZANcBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document normalization"
      ],
      "metadata": {
        "id": "qazg-OvdQMeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's left to do is to compute the normalization factors for each document. These will be used when computing the ranking score. Given the `tf` structure we can loop through it and add all the squares of the term frequencies found in each document. We then need to take the square root of the value accumulated. This can be done as follows: "
      ],
      "metadata": {
        "id": "kmu4t1IaNi-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalisation(tf):\n",
        "    norms = defaultdict(float)\n",
        "    # Loop through all words and their postings lists\n",
        "    for w, p in tf.items():\n",
        "        # for each postings list add the square of the term frequency to the entry for that document\n",
        "        for d,f in p.items():\n",
        "            norms[d] += f**2\n",
        "    \n",
        "    # Just making sure we record the sqrt of the sum of squares.\n",
        "    for d, n in norms.items():\n",
        "        norms[d] = sqrt(n)\n",
        "    return norms"
      ],
      "metadata": {
        "id": "q_BuiIcnQPop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now put everything together in an indexer function as follows:"
      ],
      "metadata": {
        "id": "3__P3XefR0YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log10,sqrt\n",
        "\n",
        "def indextextfiles_RR(path):\n",
        "    #preparing document norm dictionary\n",
        "    #counting total number of docs (needed for idf)\n",
        "    fnames = sorted(glob(path))\n",
        "    N = len(fnames)\n",
        "    #preparing and empty dict of dicts to hold term frequencies\n",
        "    tf=defaultdict(lambda : defaultdict(int))\n",
        "    # for each word w, tf[w] is the postings dictionary.\n",
        "    # i.e. a mapping from docID to a number that signifies how many times \n",
        "    # word w is contained in document docID.\n",
        "    \n",
        "    for docID,fname in enumerate(fnames):\n",
        "        # read each file into a string\n",
        "        s = readfile(fname)\n",
        "        # split string into a list of tokens\n",
        "        words = tokenize(s)\n",
        "        # for each word w, increment the corresponding entry for docID\n",
        "        # i.e. increase by one, the number of times that word w has appeared\n",
        "        # in document docID.\n",
        "        for w in (w for w in words if w!=''):\n",
        "            tf[w][docID] += 1\n",
        "            \n",
        "    # idf is a dictionary that maps each word w to log10(N/len(p))\n",
        "    # where p is the dictionary that maps docIDs to how many times w appears\n",
        "    # in the corresponding doc. If we write len(p) python will return the number\n",
        "    # of docs that contain the term w at least once (all the docIDs that have a zero count\n",
        "    # don't get counted in the len). This is exactly equal to df[w] and we can use it\n",
        "    # in the idf calculation as per the forula provided above.\n",
        "    idf = {w:log10(N/len(p)) for w,p in tf.items()}\n",
        "    \n",
        "    norms = normalisation(tf)\n",
        "    # Returning the postings, idf and norms dictionaries.\n",
        "    return tf, idf, norms"
      ],
      "metadata": {
        "id": "D6a6hlpjOpo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's apply this to our data folder:"
      ],
      "metadata": {
        "id": "n9NsQUXGR8AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf, idf, norms = indextextfiles_RR(\"docs/*.txt\")"
      ],
      "metadata": {
        "id": "-LYGQW3ATimK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating a query"
      ],
      "metadata": {
        "id": "wjlOmxVMSCCu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to start evaluating some queries. Remember, for each term `t` and for each document `d` that contains that term, we need to add  $tfidf_{t,d}$ weight to that document's score. The code below explains the whole process."
      ],
      "metadata": {
        "id": "56MYbXS-S2Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_RR(tf,idf,norms, qtext, K):\n",
        "    # We split the query text into tokens using identical processing as\n",
        "    # for the indexing stage\n",
        "    words = tokenize(qtext)\n",
        "    #initialize an empty dictionary to hold the score per document\n",
        "    scores = defaultdict(float)\n",
        "    # for each word in query, retrieve its postings\n",
        "    for w in words:\n",
        "        # for each document containing the query word, add its term frequency\n",
        "        # multiplied with the idf weight of the query word.\n",
        "        for d,TF in tf[w].i tems():\n",
        "            scores[d] += idf[w]*TF\n",
        "\n",
        "\n",
        "    # make sure we normalize the score of each document by dividing it\n",
        "    # by the norm of that document\n",
        "    for d,s in scores.items():\n",
        "        scores[d] = s/norms[d]\n",
        "    # Sort the results in reverse order (big to small)\n",
        "    # The sorted function when applied to a dictionary normally returns\n",
        "    # th dictionary keys, sorted. By supplying the key function as res.get\n",
        "    # we are forcing it to sort the dictionary by the item mapped by each key.\n",
        "    rs =sorted(res,key=res.get, reverse=True)\n",
        "    # filter out all but the top K scores\n",
        "    rs = rs[0:K]\n",
        "    # return the indices of the\n",
        "    return rs, [res[r] for r in rs]"
      ],
      "metadata": {
        "id": "Nx8QBNbpW3Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the code given above to obtain the 10 most relevant documents to the query \"england played very well\". Write your answer to the box below."
      ],
      "metadata": {
        "id": "nFa0z4LqTx4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4_iMHa-lQDpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code provided in the box below is given a string `s` and a word `q` and returns an annotaded text that has got the word `q` highlighted. To see the annotation you need to print the annotated string using the `print` command."
      ],
      "metadata": {
        "id": "OPuWkLw5URyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def highlighttext(s, q):\n",
        "    RED = \"\\033[1;30;43m\"\n",
        "    RESET = \"\\033[0;0m\"\n",
        "    REVERSE = \"\\033[;7m\"\n",
        "    words = tokenize(s)\n",
        "    qwords = tokenize(q)\n",
        "    hwords = ['**'+RED+w.upper()+RESET+'**' if w in qwords else w for w in␣\n",
        "    ,→words]\n",
        "    return \" \".join(hwords)"
      ],
      "metadata": {
        "id": "_wbyQOqFUdKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `highlighttext` together with the code shown in the previous lab for loading and printing text files in your doc folder. You can then verify that the query terms you used above do indeed appear in the returned documents. Place your answer in the box below:"
      ],
      "metadata": {
        "id": "uWF_uouwVbzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cfeTZyO9VqH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please save this as pdf by going to File->Print and select destination as \"Save to pdf\". You can then upload the file to blackboard using the submission link found next to the lab spec."
      ],
      "metadata": {
        "id": "X_Z1YoXzVwuE"
      }
    }
  ]
}