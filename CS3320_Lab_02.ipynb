{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS3320_Lab_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmS5mf/JxOlFxAud7BxM8+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvogiatzis/CS3320/blob/main/CS3320_Lab_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS3320 Lab 2. Ranked Retrieval"
      ],
      "metadata": {
        "id": "0_BiVXu8KlvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this lab we will further explore our **BBC Sports** dataset by applying some ranked retrieval algorithms. In the process we will look at TF-IDF scoring and perform some queries with free text. Our search engine should use the following formula for the tf-idf weightings\n",
        "\n",
        "$$tfidf_{t,d}=tf_{t,d}\\times\\log\\left(\\frac{N}{df_{t}}\\right)$$ \n",
        "\n",
        "where $tf_{t,d}$ is the number of times term $t$ appears in document $d$ and $df_{t}$ is the number of documents that contain term $t$ and $N$ is the total number of documents in the collection. We should also normalize your documents but not the query. \n",
        "\n",
        "Let's download the files again and unzip them in the google colab virtual computer drive."
      ],
      "metadata": {
        "id": "1kxs7o9LKs7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/gvogiatzis/CS3320/raw/main/data/bbc_sport_docs.zip\n",
        "!unzip bbc_sport_docs.zip -d docs"
      ],
      "metadata": {
        "id": "ChgJzK-b6NJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import some toolboxes again."
      ],
      "metadata": {
        "id": "FCRndErIP90N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from glob import glob\n",
        "from math import log10"
      ],
      "metadata": {
        "id": "kanUh82QPazR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We re-run the code from the previous lab that reads the files and does the tokenization. None of that needs to change."
      ],
      "metadata": {
        "id": "uAa9plZYQBoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readfile(fname):\n",
        "    f = open(fname, 'r', encoding='latin-1')\n",
        "    s = f.read()\n",
        "    f.close()\n",
        "    return s\n",
        "\n",
        "def tokenize(text):\n",
        "    DELIM = '[ \\r\\n\\t0123456789;:.,/\\(\\)\\\"\\'-]+'\n",
        "    return re.split(DELIM, text.lower())"
      ],
      "metadata": {
        "id": "yH8DLPvtCcJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Term frequencies\n",
        "\n",
        "In the Boolean Retrieval model (Lab 01) we saw how the postings datastructure was essentially a mapping from terms in the dictionary, to the document id's that contain that term. A mapping of this type can elegantly be represented in Python using a *dictionary*. So a postings dictionary such as\n",
        "    \n",
        "    {'cricket': {2, 3, 5, 7}, 'football': {0, 2, 4}, 'rugby': {1, 2, 6}}\n",
        "    \n",
        "would denote that the word *cricket* can be found in documents with id's 2, 3, 5 and 7 etc. \n",
        "\n",
        "Now the only trouble is that for ranked retrieval we need the *number of times* a particular term appears in a particular document and the postings structure described above does not provide that information. What can we do instead? \n",
        "\n",
        "For ranked retrieval we would need a datastructure which maps a term and a document to a number, i.e. how many times that term is contained in that document. This could be achieved by something like:\n",
        "\n",
        "    {('cricket',2) : 5, ('cricket',3) : 10, ('football',4):20, ('football',5):30, ('football',6):40}\n",
        "\n",
        "However we can achieve the same thing much more efficiently by a double-mapping from term to document to number. Something like the following:\n",
        "\n",
        "    {'cricket':{2:5, 3:10}, 'football':{4:20,5:30,6:40}}\n",
        "\n",
        "This is essentialy a dictionary with the terms being the keys, and the items being dictionaries themselves. These second-level dictionaries map document id's to the number of times the term appears in them. \n",
        "\n",
        "Let's say we gather a list of all the docIDs where that term appears. Note this is a list that may have duplicates arising from the same term appearing multiple times in the same document. E.g. [2,2,2,3,3,3,3,4,4]. \n",
        "\n",
        "To summarise this list using document counts, we create an empty dictionary. Thenwe go through that list of docIDs and everytime we come across a docID we havent seen before, we set the entry to one. If a docID entry exists we increment it by one. \n",
        "The result should be  {2:3, 3:4, 4:2} for the list above. This dictionary says that docid 2 has been found 3 times, docid 3 has been found 4 times and docid 4 has been found 2 times. This is what that looks like in Python:\n"
      ],
      "metadata": {
        "id": "GtEJAr2-QlO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = [2,2,2,3,3,3,3,4,4]\n",
        "\n",
        "c = dict()\n",
        "\n",
        "for x in a:\n",
        "    if x in c:\n",
        "        c[x] += 1\n",
        "    else:\n",
        "        c[x] = 1\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "id": "JxhT70AjDDGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1dc16c-f6d8-47f1-b696-a0238f81ea27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{2: 3, 3: 4, 4: 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code can be simplified somewhat by using a special datastructure known as a *defaultdict*. The difference to a normal dict is that any key that hasn't been given an entry yet, is assumed to have a default value (in our case 0). Using a defaultdict the code above would look like"
      ],
      "metadata": {
        "id": "j0NWmt9QORDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "a = [2,2,2,3,3,3,3,4,4]\n",
        "\n",
        "c = defaultdict(int)\n",
        "\n",
        "for x in a:\n",
        "    c[x] += 1\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsIsGaUqOjvE",
        "outputId": "a452e899-ef0d-41a4-d837-46cde6c12f38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {2: 3, 3: 4, 4: 2})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "which is a bit more *pythonic*. Challenge: Python `collections` has a type of data structure object called a `Counter`. Can you use a `Counter` to create the structure above given the array `a`, in a **single line of code**? Just search for `collections.Counter` to figure out the correct usage."
      ],
      "metadata": {
        "id": "_v1tKx-gOrNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hPRynAoWEj3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to create a function that will generate term frequencies for us. The structure of the code will follow that of the boolean retrieval model."
      ],
      "metadata": {
        "id": "dWbfO8mrEkrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_structure(path):\n",
        "    fnames = sorted(glob(path)) # load all filenames\n",
        "    tf=defaultdict(lambda : defaultdict(int)) \n",
        "    for docID,fname in enumerate(fnames):\n",
        "        s = readfile(fname)\n",
        "        words = tokenize(s)\n",
        "        for w in words:\n",
        "            tf[w][docID] += 1    \n",
        "    return tf"
      ],
      "metadata": {
        "id": "Jv5t34x-XN1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see this in action:"
      ],
      "metadata": {
        "id": "00Cldsw2G9SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf = create_tf_structure(\"docs/*.txt\")"
      ],
      "metadata": {
        "id": "THgQzfkMX173"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the box below, to print out the postings for the word \"incredible\"."
      ],
      "metadata": {
        "id": "LPl1qgh8F22q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-Ngi4fUyGEB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inverse document frequencies"
      ],
      "metadata": {
        "id": "qJrG1UTzIem-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = dict()\n",
        "\n",
        "for w in tf.keys():\n",
        "    df[w] = len(tf[w])"
      ],
      "metadata": {
        "id": "plhcWldSN6yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(glob('docs/*.txt'))\n",
        "df = {w:len(tf[w]) for w in tf.keys()} \n",
        "idf ={w:log10(N/len(tf[w])) for w in tf.keys()}"
      ],
      "metadata": {
        "id": "m4naZculNFAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf['unique']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P5ariZANcBe",
        "outputId": "63bf13ad-8a57-4f73-d11f-addcda69bf39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.265407496531089"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document normalization"
      ],
      "metadata": {
        "id": "qazg-OvdQMeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalisation\n",
        "    fnames = sorted(glob(path)) # load all filenames\n",
        "    tf=defaultdict(lambda : defaultdict(int)) \n",
        "    for docID,fname in enumerate(fnames):\n",
        "        s = readfile(fname)\n",
        "        words = tokenize(s)\n",
        "        for w in words:\n",
        "            tf[w][docID] += 1    \n",
        "    return tf"
      ],
      "metadata": {
        "id": "q_BuiIcnQPop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log10,sqrt\n",
        "    \n",
        "\n",
        "\n",
        "def indextextfiles_RR(path):\n",
        "    #preparing document norm dictionary\n",
        "    norms = defaultdict(float)\n",
        "    #counting total number of docs (needed for idf)\n",
        "    fnames = sorted(glob(path))\n",
        "    N = len(fnames)\n",
        "    #preparing and empty dict of dicts to hold term frequencies\n",
        "    tf=defaultdict(lambda : defaultdict(int))\n",
        "    # for each word w, tf[w] is the postings dictionary.\n",
        "    # i.e. a mapping from docID to a number that signifies how many times \n",
        "    # word w is contained in document docID.\n",
        "    \n",
        "    \n",
        "    for docID,fname in enumerate(fnames):\n",
        "        # read each file into a string\n",
        "        s = readfile(fname)\n",
        "        # split string into a list of tokens\n",
        "        words = tokenize(s)\n",
        "        # for each word w, increment the corresponding entry for docID\n",
        "        # i.e. increase by one, the number of times that word w has appeared\n",
        "        # in document docID.\n",
        "        for w in (w for w in words if w!=''):\n",
        "            tf[w][docID] += 1\n",
        "            \n",
        "    # idf is a dictionary that maps each word w to log10(N/len(p))\n",
        "    # where p is the dictionary that maps docIDs to how many times w appears\n",
        "    # in the corresponding doc. If we write len(p) python will return the number\n",
        "    # of docs that contain the term w at least once (all the docIDs that have a zero count\n",
        "    # don't get counted in the len). This is exactly equal to df[w] and we can use it\n",
        "    # in the idf calculation as per the forula provided above.\n",
        "    idf = {w:log10(N/len(p)) for w,p in tf.items()}\n",
        "    \n",
        "    # we just loop through all words and the corresponding postings dictionary\n",
        "    # We add the squares of all term frequencies to the corresponding document \n",
        "    # in the norms dictionary. \n",
        "    for w, p in tf.items():\n",
        "        for d,f in p.items():\n",
        "            norms[d] += f**2\n",
        "    \n",
        "    # Just making sure we record the sqrt of the sum of squares.\n",
        "    for d, n in norms.items():\n",
        "        norms[d] = sqrt(n)\n",
        "        \n",
        "    # Returning the postings, idf and norms dictionaries.\n",
        "    return tf, idf, norms"
      ],
      "metadata": {
        "id": "D6a6hlpjOpo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf, idf, norms = indextextfiles_RR(\"docs/*.txt\")"
      ],
      "metadata": {
        "id": "-LYGQW3ATimK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf[\"cricket\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EssXK8ZvWfdw",
        "outputId": "96d9e280-dff8-4d98-f2fc-87b439c8612c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9380485621447587"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Nx8QBNbpW3Sl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}