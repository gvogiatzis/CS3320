{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS3320_Lab_05.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Pkc2J4iXZuapMj_UYk7yZKNgqlX-1u0h","authorship_tag":"ABX9TyPfjYANXCKVctuPo/aISVM3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**CS3320 Lab 5. Document Clustering**"],"metadata":{"id":"Ip892wRmvfAa"}},{"cell_type":"markdown","source":["In this lab, we will learn how to cluster a set of documents using Python. My motivating example is to identify the latent structures within the synopses of the top 100 films of all time. We will be performing the following steps in order to achieve the clustering.\n","*   tokenizing and stemming each synopsis\n","*   transforming the corpus into vector space using tf-idf\n","*   calculating cosine distance between each document as a measure of similarity\n","*   clustering the documents using the k-means algorithm\n","*   using multidimensional scaling to reduce dimensionality within the corpus\n","*   plotting the clustering output using matplotlib\n"],"metadata":{"id":"Olq3ZgzewSaM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHAbSRVgVjmn"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import re\n","import os\n","import codecs\n","from sklearn import feature_extraction\n","from bs4 import BeautifulSoup\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.manifold import MDS\n","import joblib\n","\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import requests\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"markdown","source":["Let's download the dataset"],"metadata":{"id":"36uOnU6FwTMz"}},{"cell_type":"code","source":["!wget https://github.com/gvogiatzis/CS3320/raw/main/data/Lab5.zip\n","!unzip Lab5.zip -d docs"],"metadata":{"id":"5Qe_YflzwV7t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have three primary lists:\n","'titles': the titles of the films in their rank order\n","'wiki synopses': the synopses of the films from wiki matched to the 'titles' order\n","'imdb synopses': the synopses of the films from imdb matched to the 'titles' order\n","\n","The reading from file code is pretty simple - similar to previous workshops:"],"metadata":{"id":"COcaZR9Czj9S"}},{"cell_type":"code","source":["#import three lists: titles, wikipedia synopses and imdb synopses\n","#by reading the data from files\n","#ensure that we are reading only the first 100 records.\n","titles = open('docs/Lab5/title_list.txt').read().split('\\n')\n","#ensures that only the first 100 are read in\n","titles = titles[:100]\n","\n","synopses_wiki = open('docs/Lab5/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n","synopses_wiki = synopses_wiki[:100]\n","\n","synopses_clean_wiki = []\n","for text in synopses_wiki:\n","    text = BeautifulSoup(text, 'html.parser').getText()\n","    #strips html formatting and converts to unicode\n","    synopses_clean_wiki.append(text)\n","\n","synopses_wiki = synopses_clean_wiki\n","\n","synopses_imdb = open('docs/Lab5/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n","synopses_imdb = synopses_imdb[:100]\n","\n","synopses_clean_imdb = []\n","\n","for text in synopses_imdb:\n","    text = BeautifulSoup(text, 'html.parser').getText()\n","    #strips html formatting and converts to unicode\n","    synopses_clean_imdb.append(text)\n","\n","synopses_imdb = synopses_clean_imdb\n","\n","print(str(len(titles)) + ' titles')\n","print(str(len(synopses_wiki)) + 'wiki synopses')\n","print(str(len(synopses_imdb)) + 'imdb synopses')"],"metadata":{"id":"R8MkKa1Azw9i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After reading the data from files, we are cleaning the synopses (wiki and imdb both) using BeautifulSoup. BeautifulSoup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.\n","\n","Complete the following task: extract the Aston University wiki page (https://en.wikipedia.org/wiki/Aston_University), get the page content using BeautifulSoup, and print the text. You can get the page using the following command:\n","page = requests.get(YOURLINK). Store the page text in astonText variable.\n","  "],"metadata":{"id":"6GuoHcPf2vA5"}},{"cell_type":"code","source":[""],"metadata":{"id":"7Mfgkr2i4hAj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, Let's combine the film wiki and imdb synopses and generate the rank for each."],"metadata":{"id":"4sMTnz1L6-wu"}},{"cell_type":"code","source":["synopses = []\n","\n","for i in range(len(synopses_wiki)):\n","    item = synopses_wiki[i] + synopses_imdb[i]\n","    synopses.append(item)\n","\n","# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n","ranks = []\n","for i in range(0,len(titles)):\n","    ranks.append(i)"],"metadata":{"id":"pKT98Zry81xu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will need to remove the stopwords and stemmer from synopses, therefore, we will be using the last workshop code."],"metadata":{"id":"iOXt6HKn85pf"}},{"cell_type":"code","source":["# load nltk's English stopwords as variable called 'stopwords'\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","# load nltk's SnowballStemmer as variabled 'stemmer'\n","from nltk.stem.snowball import SnowballStemmer\n","stemmer = SnowballStemmer(\"english\")\n","\n","def tokenize_and_stem(text):\n","    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n","    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","    filtered_tokens = []\n","    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n","    for token in tokens:\n","        if re.search('[a-zA-Z]', token):\n","            filtered_tokens.append(token)\n","    stems = [stemmer.stem(t) for t in filtered_tokens]\n","    return stems\n","\n","def tokenize_only(text):\n","    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n","    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n","    filtered_tokens = []\n","    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n","    for token in tokens:\n","        if re.search('[a-zA-Z]', token):\n","            filtered_tokens.append(token)\n","    return filtered_tokens"],"metadata":{"id":"XBoqwIqM9Kdx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's call the function to process the synopses and store the proccessed (filtered) synopses in data frame."],"metadata":{"id":"J16mlxku-NIY"}},{"cell_type":"code","source":["totalvocab_stemmed = []\n","totalvocab_tokenized = []\n","for i in synopses:\n","  allwords_stemmed = tokenize_and_stem(i)\n","  totalvocab_stemmed.extend(allwords_stemmed)\n","    \n","  allwords_tokenized = tokenize_only(i)\n","  totalvocab_tokenized.extend(allwords_tokenized)\n","\n","vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"],"metadata":{"id":"dVhHDRII-FvU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Complete the following task: remove the stopwords and stemmers from the astonText (This variable contains the Aston wiki page text).  "],"metadata":{"id":"uysWHwo79n1N"}},{"cell_type":"code","source":[""],"metadata":{"id":"9VKZbYS2_VnP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Tf-idf and document similarity** \n","We will be using frequency-inverse document frequency (tf-idf) vectorizer parameters and then convert the synopses list into a tf-idf matrix. Please refer to Lab 02 if you are unsimilar with Tf-idf."],"metadata":{"id":"BV41kTvd_nbj"}},{"cell_type":"code","source":["tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n","                                 min_df=0.2, stop_words='english',\n","                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n","\n","tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n","\n","print(tfidf_matrix.shape)\n","\n","terms = tfidf_vectorizer.get_feature_names()\n","\n","dist = 1 - cosine_similarity(tfidf_matrix)"],"metadata":{"id":"oTrucuQ7AfYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["K-means clustering: Using the tf-idf matrix, we can run a slew of clustering algorithms to better understand the hidden structure within the synopses. We will be using k-means algorithm with cluster size 5. Each observation is assigned to a cluster based on the cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence."],"metadata":{"id":"NxmDLRzoAsHf"}},{"cell_type":"code","source":["num_clusters = 5\n","km = KMeans(n_clusters=num_clusters)\n","\n","km.fit(tfidf_matrix)\n","\n","clusters = km.labels_.tolist()\n","\n","joblib.dump(km,'doc_cluster.pkl')\n","km = joblib.load('doc_cluster.pkl')\n","clusters = km.labels_.tolist()\n","\n","films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters }\n","\n","frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster'])"],"metadata":{"id":"szau--O5BV-I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's display the number of films per cluster."],"metadata":{"id":"LBf_8mWPB02Q"}},{"cell_type":"code","source":["frame['cluster'].value_counts()"],"metadata":{"id":"4P_2pHoFB1b4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's groupby cluster for aggregation purposes and display the average rank (1 to 100) per cluster."],"metadata":{"id":"lIshCNGSCEyI"}},{"cell_type":"code","source":["grouped = frame['rank'].groupby(frame['cluster'])\n","grouped.mean()"],"metadata":{"id":"tgdqCQyvCFJ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's identify the top n (here we are using 6) words per cluster that are nearest to the cluster centroid and display them. These words gives a good sense of the main topic of the cluster."],"metadata":{"id":"hL--AP_5Cq_7"}},{"cell_type":"code","source":["from __future__ import print_function\n","\n","print(\"Top terms per cluster:\")\n","print()\n","order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n","\n","for i in range(num_clusters):\n","    print(\"Cluster %d words:\" % i, end='')\n","    for ind in order_centroids[i, :6]:\n","        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n","    print()\n","    print()\n","    print(\"Cluster %d titles:\" % i, end='')\n","    for title in frame.loc[i]['title'].values.tolist():\n","        print(' %s,' % title, end='')\n","    print()\n","    print()\n"],"metadata":{"id":"X2hU5uy86_aj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Multidimensional scaling (MDS) is a technique that creates a map displaying the relative positions of a number of objects, given only a table of the distances between them. We have calculated the distance of each film, let's use it and display the information in graphical form."],"metadata":{"id":"StsfNoQ0DJ1D"}},{"cell_type":"code","source":["#set up colors per clusters using a dict\n","cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n","\n","#set up cluster names using a dict\n","cluster_names = {0: 'Family, home, war', \n","                 1: 'Police, killed, murders', \n","                 2: 'Father, New York, brothers', \n","                 3: 'Dance, singing, love', \n","                 4: 'Killed, soldiers, captain'}\n","\n","MDS()\n","\n","# two components as we're plotting points in a two-dimensional plane\n","# \"precomputed\" because we provide a distance matrix\n","# we will also specify `random_state` so the plot is reproducible.\n","mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n","\n","pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n","\n","xs, ys = pos[:, 0], pos[:, 1]\n","\n","#create data frame that has the result of the MDS plus the cluster numbers and titles\n","df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n","\n","#group by cluster\n","groups = df.groupby('label')\n","\n","# set up plot\n","fig, ax = plt.subplots(figsize=(17, 9)) # set size\n","ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n","\n","#iterate through groups to layer the plot\n","#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n","for name, group in groups:\n","    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n","    ax.set_aspect('auto')\n","    ax.tick_params(\\\n","        axis= 'x',          # changes apply to the x-axis\n","        which='both',      # both major and minor ticks are affected\n","        bottom='off',      # ticks along the bottom edge are off\n","        top='off',         # ticks along the top edge are off\n","        labelbottom='off')\n","    ax.tick_params(\\\n","        axis= 'y',         # changes apply to the y-axis\n","        which='both',      # both major and minor ticks are affected\n","        left='off',      # ticks along the bottom edge are off\n","        top='off',         # ticks along the top edge are off\n","        labelleft='off')\n","    \n","ax.legend(numpoints=1)  #show legend with only 1 point\n","\n","#add label in x,y position with the label as the film title\n","for i in range(len(df)):\n","    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=8)     \n","    \n","plt.show() #show the plot"],"metadata":{"id":"CzEtjUV1DKQN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write the code to save the graph in file"],"metadata":{"id":"T_6rilOhDOXf"}},{"cell_type":"code","source":[""],"metadata":{"id":"z20rS2P4DUPx"},"execution_count":null,"outputs":[]}]}